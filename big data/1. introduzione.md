### Data Mining
Si vogliono risolvere problemi su **Data Set** grandi e complessi.

*Un problema si risolve in 2 stage*:
* stage 1: strutturo il Data Set usando un **modello** o una **struttura** $I$, su cui posso definire una task $T$ che si risolve efficientemente.
* step 2: cerco l'**algoritmo** $T_p$ che risolve $T$

> [!note] data mining
> si vuole modellare i dati in modo che sia semplice ed efficiente estrarre i dati!

#### es: phishing
Si vuole classificare le email in due sottoinsiemi: **Phishing** e **Non-Phishing**. Possiamo dare dei pesi alle parole, in modo che quelle di phishing abbiano peso positivo, dunque il problema una volta modellato, si risolve calcolando il peso della mail.

> **Nota**: Il peso calcolato, e' proporzionale alla **probabilita** che la mail sia phishing.

#### Modellazione dei dati
> [!note] Modello Statistico
> Si applica una **distribuzione** su $D$ **raw data set**.

> [!note] Adversarial Model
> L'informazione target e' gestita da un'**adversarial source** che **genera dati** con l'obiettivo di minimizzare l'efficienza algoritmica della soluzione o al caso peggiore, ne riduce la validita.

### Machine Learning
Il Machine Learning e' un'approccio al Data Mining dove si usa il Data Set come **Training Set** per allenare sistemi ML.

Si usa il Machine Learning quando non e' possibile definire una **funzione obiettivo** sul data set, quando per esempio: *il problema da risolvere non si risolve implicitamente guardando i dati forniti*.

> [!note] netflix problem
> Vogliamo predire l'user rating su un film, si risolve con ML. 
> Intuitivamente: non esiste una regola che possiamo usare per calcolare l'user rating dato un film in input!

> **Nota**: ML non conviene quando esiste una funzione obiettivo ben definita sul Data Set. Per esempio trovare dei `CV` sul WEB, o `trovare triangoli` sui social network e' risolvibile senza ML.

### Limite statistico del data mining
In Data Mining, di solito si vuole trovare **eventi inusuali** in grandi **Data Set**

> [!note] principio di bonferroni
> Se il set di dati raccoglie un gran numero di dati da fonti in modo **sufficientemente casuale** allora gli **eventi insoliti** potrebbero non avere significato ed essere quindi solo **artefatti statistici** che possono verificarsi.

Supponendo che io stia cercando uno specifico evento $E$ nel Data Set $DS$, allora mi posso aspettare che $E$ occorra, anche se $DS$ e' completamente randomico,e il numero di occorrenze di $E$ aumenta all'aumentare di $DS$.

> [!note] bogus
> Queste occorrenze in piu che troviamo (bogus) sembrano inusuali e significanti ma in verita non lo sono!

#### es: esempio su bonferroni
Ci sono 1 miliardo di persone, tra questi ci sono i cattivi. Ognuno di questi va in un hotel, ogni hotel ha una capienza di 100 persone (dunque ci sono 100.000 hotel) e vogliamo esaminare i record di questi hotel su un periodo di 1000 giorni.

Vediamo come si comportano coppie di due persone in questo esempio:
* la probabilita di visitare un hotel in un determinato giorno e' di $$

###  A Computational Lens for Data Mining
Voglio rispondere a query complesse su un Data Set, per esempio: data una stream `DS` di interi, voglio mantenere la **media** del `DS` e la **deviazione standard** (il numero piu strano ???).

> **Nota:** L'algoritmo non dovrebbe basarsi su **ipotesi statistiche**.

### Data Streams
Quando non conosciamo in anticipo la struttura del Data Set allora ci troviamo a dover gestire uno **stream** il cui input rate e' gestito esternamente. 

> **Nota**: il data set e' dunque "infinito" e non-stazionario, nel senso che la distribuzione cambia nel tempo.

Gli elementi in input entrano ad un **rapid rate**, dove gli elementi della stream sono $\text{t-tuple}$,  e solo alcuni **sketches** della stream $S$ possono essere tenuti in memoria (**memoria limitata**)

Su una datastream posso fare queste query:
* **sampling**: costruire un **sample random**
* **filtering**: selezionare elementi con proprieta $x$
* **counting**: numero di element distinti negli ultimi $k$ elementi
* **estimating moments**: average e deviazione standard degli ultimi $k$ elementi
* **finding**: trovare i $k$ piu frequenti

### riassumere le informazioni: page rank
> [!note] sketch $H(S)$
> Dato un large data set, voglio $H(S)$ che riassume le **feature principali** di DS dove:
> * $|H(S)|$ e' minimizzato asintoticamente rispetto a DS
> * $H(S)$ approssima bene DS
> * $H(S)$ e' una struttura dinamica efficiente
> 

L'algoritmo di page-rank riassume il `web` usando il `rank` value per ogni pagina $x$:
* $rank(..) \sim \text{ Probability Distribuzione di un Random Walk sul WEB graph}$ 
* $rank(x)$ misura in modo efficace l'importanza di una pagina $x$ sull'intero web.


